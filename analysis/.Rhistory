d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
head(d)
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1,]
upper <- Int89[2,]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[,1]
upper <- Int89[,2]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
temp <- post$a + post$b * 50
HDPI(temp)
HPDI(temp)
HPDI(temp)[1]
HPDI(temp)[2]
head(d)
d
data(islandsDistMatrix)
Dmat <- islandsDistMatrix
library(rethinking)
data(islandsDistMatrix)
Dmat <- islandsDistMatrix
data("Kline2")
d <- Kline2
# tidy
rm(Kline2)
# prep indexes
d$society <- 1:10
m13.7 <- map2stan(
alist(
total_tools ~ dpois(lambda),
log(lambda) <- a + g[society] + bp*logpop,
g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ),
a ~ dnorm(0,10),
bp ~ dnorm(0,1),
etasq ~ dcauchy(0,1),
rhosq ~ dcauchy(0,1)
),
data=list(
total_tools=d$total_tools,
logpop=d$logpop,
society=d$society,
Dmat=islandsDistMatrix),
warmup=2000 , iter=1e4 , chains=4)
precis(m13.7,depth=2)
post <- extract.samples(m13.7)
curve(median(post$etasq)*exp(-median(post$rhosq)*x^2), from = 0, to = 10,
xlab = "dist (thousand km", ylab = "covariance", ylim = c(0,1),
yaxp = c(0,1,4), lwd=2)
curve(median(post$etasq)*exp(-median(post$rhosq)*x^2), from = 0, to = 10,
xlab = "dist (thousand km", ylab = "covariance", ylim = c(0,1),
yaxp = c(0,1,4), lwd=2)
# plot 100 function samples
for( i in 1:100)
curve(post$etasq[i]*exp(-post$rhosq[i]*x^2), add = T,
col = col.alpha("black", 0.2))
dev.off()
K <- matrix(0, nrow = 10, ncol = 10)
for(i in 1:10)
for(j in 1:10)
K[i,j] <- median(post$etasq) *
exp(-median(post$rhosq) *
islandsDistMatrix[i,j]^2)
diag(K) <- median(post$etasq) + 0.01
Rho <- round(cov2cor(K), 2)
# add names for convenience
colnames(Rho) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
rownames(Rho) <- colnames(Rho)
Rhow
Rho
psize <- d$logpop/max(d$logpop)
psize <- exp(psize*1.5)-2
# plot raw data and labels
plot(d$lon2, d$lat, xlab = "longitude", ylab = "latitude",
col = randi2, cex = psize, pch = 16, xlim = c(-50,30))
labels <- as.character(d$culture)
text(d$lon2, d$lat, labels = labels, cex = 0.7, pos = c(2,4,3,3,4,1,3,2,4,2))
# overlay lines
for(i in 1:10)
for(j in 1:10)
if(i < j)
lines(c(d$lon2[i],d$lon2[j]), c(d$lat[i], d$lat[j]),
lwd=2, col = col.alpha("black", Rho[i,j]^2))
psize <- d$logpop/max(d$logpop)
psize <- exp(psize*1.5)-2
# plot raw data and labels
plot(d$lon2, d$lat, xlab = "longitude", ylab = "latitude",
col = randi2, cex = psize, pch = 16, xlim = c(-50,30))
labels <- as.character(d$culture)
text(d$lon2, d$lat, labels = labels, cex = 0.7, pos = c(2,4,3,3,4,1,3,2,4,2))
# overlay lines
for(i in 1:10)
for(j in 1:10)
if(i < j)
lines(c(d$lon2[i],d$lon2[j]), c(d$lat[i], d$lat[j]),
lwd=2, col = col.alpha("black", Rho[i,j]^2))
plot(d$lon2, d$lat, xlab = "longitude", ylab = "latitude",
col = rangi2, cex = psize, pch = 16, xlim = c(-50,30))
labels <- as.character(d$culture)
text(d$lon2, d$lat, labels = labels, cex = 0.7, pos = c(2,4,3,3,4,1,3,2,4,2))
# overlay lines
for(i in 1:10)
for(j in 1:10)
if(i < j)
lines(c(d$lon2[i],d$lon2[j]), c(d$lat[i], d$lat[j]),
lwd=2, col = col.alpha("black", Rho[i,j]^2))
logpop.seq <- seq( from=6 , to=14 , length.out=30 )
lambda <- sapply( logpop.seq ,
function(lp) exp( post$a + post$bp*lp ) )
lambda.median <- apply( lambda , 2 , median )
lambda.PI80 <- apply( lambda , 2 , PI , prob=0.8 )
# plot raw data and labels
plot( d$logpop ,
d$total_tools ,
col=rangi2 ,
cex=psize ,
pch=16 ,
xlab="log population" ,
ylab="total tools" )
text( d$logpop ,
d$total_tools ,
labels=labels ,
cex=0.7 ,
pos=c(4,3,4,2,2,1,4,4,4,2) )
# display posterior predictions
lines( logpop.seq ,
lambda.median ,
lty=2 )
lines( logpop.seq ,
lambda.PI80[1,] ,
lty=2 )
lines( logpop.seq ,
lambda.PI80[2,] ,
lty=2 )
# overlay correlations
for( i in 1:10 )
for ( j in 1:10 )
if ( i < j )
lines( c( d$logpop[i],d$logpop[j] ) ,
c( d$total_tools[i],d$total_tools[j] ) ,
lwd=2 , col=col.alpha("black",Rho[i,j]^2) )
setwd("F:/Uni/PhD/1st year/Groups/Level 3/2nd Semester/Experiment")
library(tidyverse)
library(reshape)
#### Created functions ####
# normalising distnace function
norm_dist <- function(x,y){
2*(x - (y*-1))/(y - (y*-1))-1
}
#### read in the data ####
dat <- read.csv("data/Part_2/Part2_measures.txt", sep = "\t")
#### Pre-process data ####
# sort out participant coding
dat$participant <- paste(dat$experimenter, dat$participant, sep = "_")
dat$participant <- as.factor(dat$participant)
# drop experimenter column
dat <- dat[ , !(names(dat) %in% "experimenter")]
levels(dat$colour)
dat$colour[dat$colour == "y"] <- "Y"
dat$colour[dat$colour == " Y"] <- "Y"
dat$colour[dat$colour == "Y "] <- "Y"
dat$colour[dat$colour == "Y  "] <- "Y"
dat$colour[dat$colour == "Y   "] <- "Y"
unique(dat$colour)
levels(dat$colour)
dat$colour[dat$colour == "Yellow"] <- "Y"
unique(dat$colour)
dat$colout[dat$colour == "Red"] <- "R"
dat$colour[dat$colour == "Red"] <- "R"
# sort yellow
dat$colour[dat$colour == "y"] <- "Y"
dat$colour[dat$colour == " Y"] <- "Y"
dat$colour[dat$colour == "Y "] <- "Y"
dat$colour[dat$colour == "Y  "] <- "Y"
dat$colour[dat$colour == "Y   "] <- "Y"
dat$colour[dat$colour == "Yellow"] <- "Y"
# sort blue
dat$colour[dat$colour == "Blue"] <- "B"
dat$colour[dat$colour == "Blue "] <- "B"
unique(dat$colour)
# remove unused levels
dat$colour <- factor(dat$colour)
# reorder levels
dat$colour <- factor(dat$colour,
levels(dat$colour)[c(1,3,2)])
# add in hoop_position
for (row in 1:nrow(dat))
{
dat$hoop_pos[row] = dat[row,as.character(dat$colour[row])]
}
# tidy
rm(row)
# create columns to fill
dat$small_pos <- 0
dat$large_pos <- 0
#### this doesn't work just now... have a look again ####
# rownames used so it doesn't loop over the first 15 rows...
for(i in levels(dat$participant)){
a <- 10
for(x in levels(dat$colour)){
for(z in rownames(dat[dat$colour == x & dat$participant == i,])){
q <- as.numeric(z)
if(unique(dat[q,a]) == "large"){
dat$small_pos[q] <- dat$hoop_pos[q]
} else {
dat$small_pos[q] <-  dat$hoop_pos[q]*-1
}
}
a <- a + 1
}
}
# tidy
rm(a,i,q,x,z)
# add in large pos
dat$large_pos <- dat$small_pos*-1
#### load in data from part 1 ####
# part1 acc measures
load("temp/beanbagdat")
# slabs to test
load("temp/slabs_to_test")
#### normalising distance data ####
# this means we need to make all large hoops negative and standing towards these negative values too
# Also, make small hoops positive, and the standing positions closer to small hoops
# Can do this based on position data; probably separate it out...
norm_dat <- dat
# can probably loop through it?
for(i in levels(norm_dat$participant)){
for(z in rownames(norm_dat[norm_dat$large_pos > 0 & norm_dat$participant == i,])){
q <- as.numeric(z)
norm_dat$small_pos[q] <- norm_dat$small_pos[q] *-1
norm_dat$subject_position[q] <- norm_dat$subject_position[q]*-1
norm_dat$large_pos[q] <- norm_dat$large_pos[q]*-1
}
}
# tidy
rm(i, q, z)
# apply the normalising distance function
norm_dat$norm_dist <- norm_dist(norm_dat$subject_position, norm_dat$hoop_pos)
#### work out opt switching point ####
# create empty data.frame to get partcipants accuracy across all hoop separations
acc_slab <- data.frame(participant = character(),
hoop_size = character(),
slab = numeric(),
acc = numeric())
# create data.frame for just the switch point
switch_slab <- data.frame(participant = character(),
hoop_size = character(),
slab = numeric())
for(x in unique(beanbagdat$participant))
{
for(i in unique(beanbagdat$hoop_size))
{
ss = beanbagdat[beanbagdat$participant==x & beanbagdat$hoop_size==i,]
m = glm(data=ss, cbind(inhoop, trials-inhoop)~slab+0,
offset=ss$off_set, binomial)
for(a in 1:30)
{
p = predict(m, data.frame(slab=a), type="response")
p = as.numeric(p)
acc_slab <- rbind(acc_slab, data.frame(participant = x,
hoop_size = i,
slab = a,
acc = round(p[1], digits = 7)))
}
slab = max(which(predict(m, data.frame(slab=1:30), type='response')>0.5))
switch_slab <- rbind(switch_slab, data.frame(participant = x,
hoop_size = i,
slab = slab))
}
}
# tidy
rm(ss, m, a, i, p, x, slab)
# get switch points for small hoops
switch_slab_sm <- switch_slab[switch_slab$hoop_size == "small",]
switch_slab_sm <- switch_slab_sm[,c("participant",
"slab")]
colnames(switch_slab_sm) <- c("participant", "switchSlab_sm")
# now for large
switch_slab_la <- switch_slab[switch_slab$hoop_size == "large",]
switch_slab_la <- switch_slab_la[,c("participant",
"slab")]
colnames(switch_slab_la) <- c("participant", "switchSlab_la")
# get switch pos depending on average dist
# this is to reflect the point at which participants can't maintain
# an average acc of 50%
switch_slab_both <- merge(switch_slab_la, switch_slab_sm)
switch_slab_both$switchSlab <- (switch_slab_both$switchSlab_la +
switch_slab_both$switchSlab_sm)/2
switch_slab_both <- switch_slab_both[,c("participant", "switchSlab")]
# create switchSlab column in norm_dat
norm_dat <- merge(switch_slab_both, norm_dat)
# tidy
rm(switch_slab, switch_slab_la, switch_slab_sm, switch_slab_both)
#### get even acc stand dist ####
# get small distances first
temp_small <- melt(slabs_to_test[slabs_to_test$hoop_size == "small",],
id = c("participant", "hoop_size"))
temp_small <- temp_small[,c("participant",
"value")]
colnames(temp_small) <- c("participant",
"small_hoop_dist")
# now get avg dists
temp_avg <- melt(slabs_to_test[slabs_to_test$hoop_size == "avg",],
id = c("participant",
"hoop_size"))
temp_avg <- temp_avg[,c("participant",
"value")]
colnames(temp_avg) <- c("participant",
"avg_hoop_dist")
# merge data sets
temp_avg_small <- cbind(temp_avg, temp_small)
temp_avg_small <- temp_avg_small[,c("participant",
"avg_hoop_dist",
"small_hoop_dist")]
colnames(temp_avg_small) <- c("participant",
"hoop_pos",
"small_hoop_dist")
# get the amount of shift towards the small hoop
temp_avg_small$shift <- temp_avg_small$hoop_pos - temp_avg_small$small_hoop_dist
# remove small_hoop_dist
temp_avg_small <- temp_avg_small[,c("participant",
"hoop_pos",
"shift")]
# add this into norm_dat
norm_dat <- merge(temp_avg_small, norm_dat)
# reorder columns so it's clearer
norm_dat <- norm_dat[,c("participant",
"trial",
"colour",
"direction",
"subject_position",
"accuracy",
"R",
"Y",
"B",
"B_N_Size",
"Y_N_Size",
"R_N_Size",
"hoop_pos",
"small_pos",
"large_pos",
"norm_dist",
"switchSlab",
"shift")]
#### get standing position for equal accuracy ####
norm_dat$equacc <- norm_dist(norm_dat$shift, norm_dat$hoop_pos)
#### add in opt standing position ####
norm_dat$optpos <- norm_dat$equacc
norm_dat$optpos[norm_dat$hoop_pos > norm_dat$switchSlab] <- 1
#### make plot of opt swithing poing by standing position ####
# still need to calculate their opt standing positions
plt <- ggplot(norm_dat, aes(hoop_pos, norm_dist))
plt <- plt + geom_point(alpha = 0.1)
plt <- plt + geom_jitter(width = 0.3, height = 0.0)
plt <- plt + geom_line(aes(hoop_pos, equacc, colour = "Equal Accuracy"), size = 1.2, alpha = 0.7)
plt <- plt + geom_line(aes(hoop_pos, optpos, colour = "Optimal"), size = 1.2, alpha = 0.7)
plt <- plt + geom_point(aes(hoop_pos, optpos, colour = "Optimal"))
plt <- plt + geom_point(aes(hoop_pos, equacc, colour = "Equal Accuracy"))
#plt <- plt + geom_vline(xintercept = norm_dat$switchSlab)
plt <- plt + scale_y_continuous(name="Normalised Participant Position", limits=c(-1,1))
plt <- plt + scale_x_continuous(name="Distance (slabs)", limits= c(0, 21))
plt <- plt + geom_hline(yintercept = 0)
plt <- plt + facet_wrap(~participant)
plt
plt
#### level 3 - 2nd Semester 2017/18 ####
# Part 2 Standing position script
# This script looks at where participants stood on each trial and
# also where they should have stood for the optimal strategy. Also,
# it looks at where they would have stood if they were trying to
# achieve equal accuracy for both hoop sizes.
#### Load in the libraries ####
library(tidyverse)
library(reshape)
#### Created functions ####
# normalising distnace function
norm_dist <- function(x,y){
2*(x - (y*-1))/(y - (y*-1))-1
}
#### read in the data ####
dat <- read.csv("data/Part_2/Part2_measures.txt", sep = "\t")
#### Pre-process data ####
# sort out participant coding
dat$participant <- paste(dat$experimenter, dat$participant, sep = "_")
dat$participant <- as.factor(dat$participant)
# drop experimenter column
dat <- dat[ , !(names(dat) %in% "experimenter")]
# make all colour values the same
# sort red
dat$colour[dat$colour == "Red"] <- "R"
# sort yellow
dat$colour[dat$colour == "y"] <- "Y"
dat$colour[dat$colour == " Y"] <- "Y"
dat$colour[dat$colour == "Y "] <- "Y"
dat$colour[dat$colour == "Y  "] <- "Y"
dat$colour[dat$colour == "Y   "] <- "Y"
dat$colour[dat$colour == "Yellow"] <- "Y"
# sort blue
dat$colour[dat$colour == "Blue"] <- "B"
dat$colour[dat$colour == "Blue "] <- "B"
# unique order now B Y R
# remove unused levels
dat$colour <- factor(dat$colour)
# reorder levels
dat$colour <- factor(dat$colour,
levels(dat$colour)[c(1,3,2)])
# add in hoop_position
for (row in 1:nrow(dat))
{
dat$hoop_pos[row] = dat[row,as.character(dat$colour[row])]
}
# tidy
rm(row)
# create columns to fill
dat$small_pos <- 0
dat$large_pos <- 0
#### this doesn't work just now... have a look again ####
# rownames used so it doesn't loop over the first 15 rows...
for(i in levels(dat$participant)){
a <- 10
for(x in levels(dat$colour)){
for(z in rownames(dat[dat$colour == x & dat$participant == i,])){
q <- as.numeric(z)
if(unique(dat[q,a]) == "large"){
dat$small_pos[q] <- dat$hoop_pos[q]
} else {
dat$small_pos[q] <-  dat$hoop_pos[q]*-1
}
}
a <- a + 1
}
}
# tidy
rm(a,i,q,x,z)
# add in large pos
dat$large_pos <- dat$small_pos*-1
NG3 <- dat[dat$participant == "NG_3",]
View(NG3)
rm(list = ls())
library(brms)
library(rstan)
library(tidyverse)
install.packages(brms)
install.packages("brms")
library(brms)
setwd("F:/Uni/Github/TargetAbsentStoppingRules/analysis")
load("scratch/models/brm_m1")
m1_effects <- marginal_effects(m1_rt_theta)
plot(m1_effects, plot = TRUE)
plot(m1_effects, plot = TRUE) +
ggplot2::ggtitle("Model of Response Times (RT) by 'Difficulty' (Theta)") +
ggplot2::labs(y = "RT", x = "Theta")
plot(m1_effects, plot = TRUE) +
ggplot2::ggtitle("Model of Response Times (RT) by 'Difficulty' (Theta)")
plot(m1_effects, plot = FALSE)[[1]] +
ggplot2::ggtitle("Model of Response Times (RT) by 'Difficulty' (Theta)")
plot(m1_effects, plot = FALSE)[[1]] +
ggplot2::ggtitle("Model of Response Times (RT) by 'Difficulty' (Theta)") +
ggplot2::labs(y = "RT", x = "Theta")
plot(m1_effects, plot = FALSE)[[1]] +
ggplot2::ggtitle("Model of Response Times (RT) by 'Difficulty' (Theta)") +
ggplot2::theme(plot.title = element_text(hjust = 0.5)) +
ggplot2::labs(y = "RT", x = "Theta")
summary(m1_rt_theta)
plot(m1_effects, plot = FALSE)
dev.off()
plot(m1_effects, plot = FALSE)
plot(m1_effects, plot = T)
get_prior(rt ~ theta + (1 + theta|participant),
data = df)
load("scratch/processed_data_nar_TA.rda")
# turn tibble into data.frame for map
df <- as.data.frame(df_TA)
# tidy
rm(df_TA)
get_prior(rt ~ theta + (1 + theta|participant),
data = df)
methods(class = "brmsfit")
library(shinystan)
launch_shinystan(m1_rt_theta)
rm(list = ls())

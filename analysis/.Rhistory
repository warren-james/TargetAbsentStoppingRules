# we couldn't tell from the precis output
# Put the map values over the actual data
plot(height ~ weight, data = d2)
abline(a=coef(m4.3)["a"], b = coef(m4.3)["b"])
# using coef extracts a value from the model that was fitted
# in this case, it gets the intercept and slope values
# However, the above does a poor job of explaning the
# the is inherent in any model like this
# one way to do this is get a bunch of lines that are from
# samples in the model
post <- extract.samples(m4.3)
# the exact values aren't that important, but it gives us
# an idea of the "spread" and therefore the uncertainty. the
# average of all these should give us the MAP values
# For this part, we will use progressively more data each time
# so we can see how that effects the scatter of the lines
N <- 10 # change this number to see how predictions change
dN <- d2[1:N, ]
mN <- map(
alist(
height ~ dnorm(mu, sigma),
mu <- a + b*weight,
a ~ dnorm(178, 100),
b ~ dnorm(0, 10),
sigma ~ dunif(0,50)
),
data = dN)
# now let's extract samples and plot 20 of these lines
post <- extract.samples(mN, n = 20)
# display data
plot(dN$weight, dN$height,
xlim = range(dN$weight), ylim = range(dN$height),
col = rangi2)
# plot lines with transparency
for( i in 1:20 )
abline(a = post$a[i], b = post$b[i], col = col.alpha("black" , 0.3))
# plotting regression intervals and contours
# this part will just show you how to do that. Think about one
# value for weight (say 50kg), we can get a bunch of values for
# mu for someone of this weight
# redefine post as this first
post <- extract.samples(m4.3)
# now get some example values
mu_at_50 <- post$a + post$b * 50
# so what does the shape of this look like?
dens(mu_at_50, col = rangi2, lwd = 2)
# let's look for an interval in this
HPDI(mu_at_50, prob = 0.89)
# this is great... but long winded if we had to do this
# for every weight value in the model...
# so we can use the "link()" function to save some time
mu <- link(m4.3)
str(mu)
# all this does is take the model, and compute the mu value
# for each case in the data
# So this has give us a distribution of mu for each individual
# but we want one for each unique weight, so we do something
# a bit different
# define seq of weights
weight.seq <- seq(from = 25, to = 70, by = 1)
# use link()
mu <- link(m4.3, data = data.frame(weight = weight.seq))
str(mu)
# time to visualise this
plot(height ~ weight, d2, type="n")
# loop over samples and plot mu values
for(i in 1:100)
points(weight.seq, mu[i], pch = 16, col = col.alpha(rangi2,0.1))
for(i in 1:100)
points(weight.seq, mu[i,], pch = 16, col = col.alpha(rangi2,0.1))
mu.mean <- apply(mu, 2, mean)
mu.HPDO <- apply(mu, 2, HDPI, prob=0.89)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)
mu,mea
mu.mean
mu.HPDI
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))
# plot MAP line
lines(weight.seq, mu.mean)
# add in shaded region
shade(mu.HPDI, weight.seq)
sim.height <- sim(m4.3, data = list(weight = weight.seq))
str(sim.height)
height.PI <- apply(sim.heigh, 2, PI, prob = 0.89)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
plot(height~weight, d2, col = col.alpha(rangi2, 0.5))
# draw MAP
lines(weight.seq, mu.mean)
# drar HPDI region
shade(mu.HPDI, weight.seq)
# drar PI region for simulated heights
shade(height.PI, weight.seq)
rm(list = ls())
data(Howell1)
d <- Howell1
str(d)
plot(height~weight)
plot(height~weight, d)
d$weight.s <- (d$weight - mean(d$weight))/sd(d$weight)
mean(d$weight.s)
d$weight.s2 <- d$weight.s^2
m4.5 <- map(
alist(
height ~ dnorm(mu, sigma),
mu <- a + b1*weight.s + b2*weight.s2 ,
a ~ dnorm(178, 100),
b1 ~ dnorm(0, 10),
b2 ~ dnorm(0, 10),
sigma ~ dunif(0, 50)
),
data = d)
precis(m4.5)
weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight.s=weight.seq, weight.s2=weight.seq^2)
mu <- link(m4.5, data = pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
sim.height <- sim(m4.5, data = pred_dat)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
plot(height ~ weight.s, d, col = col.alpha(rani2,0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
plot(height ~ weight.s, d, col = col.alpha(rangi2,0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
d$weight.s3 <- d$weight.s^3
# add in the extra term again
m4.6 <- mapply(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b1*weight.s + b2*weight.s2 + b3*weight.s3,
a ~ dnorm(178,100),
b1 ~ dnorm(0,10),
b2 ~ dnorm(0,10),
b3 ~ dnorm(0,10),
sigma ~ dunif(0,50)
),
data = d)
m4.6 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b1*weight.s + b2*weight.s2 + b3*weight.s3,
a ~ dnorm(178,100),
b1 ~ dnorm(0,10),
b2 ~ dnorm(0,10),
b3 ~ dnorm(0,10),
sigma ~ dunif(0,50)
),
data = d)
test <- replicate(1e4, prod(0, 0, 10))
test
test <- replicate(1e4, prod(12, 0, 10))
head(test)
sim_mu <- rnorm(1e4, 0, 10)
sim_sig <- runif(1e4, 0, 10)
sim_prior <- rnorm(1e4, sim_mu, sim_sig)
sim_mu <- rnorm(1e4, 0, 10)
sim_sig <- runif(1e4, 0, 10)
sim_prior <- rnorm(1e4, sim_mu, sim_sig)
dens(sim_prior)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18,]
rm(d)
d2$weight.s <- (d2$weight - mean(d2$weight)) / sd(d2$weight)
m4.4 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.c,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
m4.4 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.s,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.s,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
post <- extract.samples(m4.H1)
mu_at_50 <- post$a + post$b * 50
mu_at_50
d2$weight.c <- d2$weight - mean(d2$weight)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.c,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
# now let's extract some stuff
post <- extract.samples(m4.H1)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.c,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
post <- extract.samples(m4.H1)
mu_at_50 <- post$a + post$b * 50
mean(mu_at_50)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
# now let's extract some stuff
post <- extract.samples(m4.H1)
mu_at_50 <- post$a + post$b * 50
mean(mu_at_50)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight,
a ~ dnorm(178,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
# now let's extract some stuff
post <- extract.samples(m4.H1)
mu_at_50 <- post$a + post$b * 50
mean(mu_at_50)
weights_to_check <- c(46.95, 43.72, 64.78, 32.59, 54.63)
d = NULL
d = rbind(d, data.frame(weights_to_check))
d
for(i in weights_to_check)
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
d = rbind(d, data.frame(weights_to_check, Expected_height, Int89))
rm(d)
d = NULL
for(i in weights_to_check)
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
d = rbind(d, data.frame(weights_to_check, Expected_height, Int89))
temp <- post$a + post$b * 50
Int89 <- HPDI(temp, prob = 0.89)
Int89
Int89[1]
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
d = rbind(d, data.frame(weights_to_check, Expected_height, Int89[1], Int89[2]))
}
rm(d)
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
d = rbind(d, data.frame(weights_to_check, Expected_height, lower, upper))
}
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
d = rbind(d, data.frame(weights_to_check[i], Expected_height, lower, upper))
}
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
d = rbind(d, data.frame(i, Expected_height, lower, upper))
}
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
d = rbind(d, data.frame(i, Expected_height, lower, upper))
}
head(d)
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
head(d)
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1,]
upper <- Int89[2,]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[,1]
upper <- Int89[,2]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
temp <- post$a + post$b * 50
HDPI(temp)
HPDI(temp)
HPDI(temp)[1]
HPDI(temp)[2]
head(d)
d
setwd("F:/Uni/Github/TargetAbsentStoppingRules/analysis")
load("scratch/models/m_ta_only_temp_2")
precis(m_ta_only_temp_2)
precis(m_ta_only_temp_2, depth=2)
stancode(m_ta_only_temp_2)
data(islandsDistMatrix)
library(rethinking)
data(islandsDistMatrix)
rm(islandsDistMatrix)
data(islandsDistMatrix)
Dmat <- islandsDistMatrix
# tidy
rm(islandsDistMatrix)
data("Kline2")
d <- Kline2
rm(Kline2)
m13.7 <- map2stan(
alist(
total_tools ~ dpois(lambda),
log(lambda) <- a + g[society] + bp*logpop,
g[society] ~ GPL2(Dmat, etasq, rhosq, 0.01) # 0.01 here would normally be a prior
# but because we only have 1 obs it's ok
a ~ dnorm(0,10),
bp ~ dnorm(0,1),
etasq ~ dcauchy(0,1),
rhosq ~ dcauchy(0,1)
),
data = list(
total_tools = d$total_tools,
logpop = d$logpop,
society = d$society,
Dmat = Dmat),
warmup = 2000, iter=1e4, chains = 4)
m13.7 <- map2stan(
alist(
total_tools ~ dpois(lambda),
log(lambda) <- a + g[society] + bp*logpop,
g[society] ~ GPL2(Dmat, etasq, rhosq, 0.01), # 0.01 here would normally be a prior
# but because we only have 1 obs it's ok
a ~ dnorm(0,10),
bp ~ dnorm(0,1),
etasq ~ dcauchy(0,1),
rhosq ~ dcauchy(0,1)
),
data = list(
total_tools = d$total_tools,
logpop = d$logpop,
society = d$society,
Dmat = Dmat),
warmup = 2000, iter=1e4, chains = 4)
m13.7 <- map2stan(
alist(
total_tools ~ dpois(lambda),
log(lambda) <- a + g[society] + bp*logpop,
g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ),
a ~ dnorm(0,10),
bp ~ dnorm(0,1),
etasq ~ dcauchy(0,1),
rhosq ~ dcauchy(0,1)
),
data=list(
total_tools=d$total_tools,
logpop=d$logpop,
society=d$society,
Dmat=Dmat),
warmup=2000 , iter=1e4 , chains=4)
data(islandsDistMatrix)
m13.7 <- map2stan(
alist(
total_tools ~ dpois(lambda),
log(lambda) <- a + g[society] + bp*logpop,
g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ),
a ~ dnorm(0,10),
bp ~ dnorm(0,1),
etasq ~ dcauchy(0,1),
rhosq ~ dcauchy(0,1)
),
data=list(
total_tools=d$total_tools,
logpop=d$logpop,
society=d$society,
Dmat=islandsDistMatrix),
warmup=2000 , iter=1e4 , chains=4)
setwd("E:/Github/TargetAbsentStoppingRules/analysis")
library(rethinking)
rstan_options(auto_write = TRUE)
#### TA judgement model ####
# load in data
load("scratch/processed_data_nar_TA.rda")
# turn tibble into data.frame for map
df <- as.data.frame(df_TA)
# tidy
rm(df_TA)
m_ta_only_temp_4 <- map2stan(
alist(
rt ~ dlnorm(mu, sigma),
mu <-  A  + B_theta + B_blk + B_blk_th,
A <- a + a_p[participant],
B_theta <- (b_theta + b_theta_p[participant]) * theta,
B_blk <- b_issi*issi + b_isra*isra,
B_blk_th <- (b_si_theta*issi + b_ra_theta*isra) * theta,
# # adaptive priors
c(a_p, b_theta_p)[participant] ~ dmvnormNC(sigma_p, Rho),
# fixed priors
a ~ dnorm(1, 3),
b_theta ~ dnorm(1,3),
b_isra ~ dnorm(1,3),
b_issi ~ dnorm(1,3),
b_ra_theta ~ dnorm(1,3),
b_si_theta ~ dnorm(1,3),
sigma ~ dcauchy(0, 1),
sigma_p ~ dcauchy(0, 1),
Rho ~ dlkjcorr(4)
),
data = df,
iter = 2000, warmup = 1000, chains = 3, cores = 3)
precis(m_ta_only_temp_4)
save(m_ta_only_temp_4, file = "scratch/models/m_ta_only_temp_4")
setwd("E:/Github/TargetAbsentStoppingRules/analysis")
library(rethinking)
rstan_options(auto_write = TRUE)
#### TA judgement model ####
# load in data
load("scratch/processed_data_nar_TA.rda")
# turn tibble into data.frame for map
df <- as.data.frame(df_TA)
# tidy
rm(df_TA)
m_ta_only_temp_4 <- map2stan(
alist(
rt ~ dlnorm(mu, sigma),
mu <-  A  + B_theta + B_blk + B_blk_th,
A <- a + a_p[participant],
B_theta <- (b_theta + b_theta_p[participant]) * theta,
B_blk <- b_issi*issi + b_isra*isra,
B_blk_th <- (b_si_theta*issi + b_ra_theta*isra) * theta,
# # adaptive priors
c(a_p, b_theta_p)[participant] ~ dmvnormNC(sigma_p, Rho),
# fixed priors
a ~ dnorm(0.55,1),
b_theta ~ dnorm(1,3),
b_isra ~ dnorm(0,1),
b_issi ~ dnorm(0,1),
b_ra_theta ~ dnorm(0,1),
b_si_theta ~ dnorm(0,1),
sigma ~ dcauchy(0,1),
sigma_p ~ dcauchy(0,1),
Rho ~ dlkjcorr(4)
),
data = df,
iter = 2000, warmup = 1000, chains = 3, cores = 3)
precis(m_ta_only_temp_4)
exp(0.24)
save(m_ta_only_temp_4, file = "scratch/models/m_ta_only_temp_4")

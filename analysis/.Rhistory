mu <- link(m4.3)
str(mu)
# all this does is take the model, and compute the mu value
# for each case in the data
# So this has give us a distribution of mu for each individual
# but we want one for each unique weight, so we do something
# a bit different
# define seq of weights
weight.seq <- seq(from = 25, to = 70, by = 1)
# use link()
mu <- link(m4.3, data = data.frame(weight = weight.seq))
str(mu)
# time to visualise this
plot(height ~ weight, d2, type="n")
# loop over samples and plot mu values
for(i in 1:100)
points(weight.seq, mu[i], pch = 16, col = col.alpha(rangi2,0.1))
for(i in 1:100)
points(weight.seq, mu[i,], pch = 16, col = col.alpha(rangi2,0.1))
mu.mean <- apply(mu, 2, mean)
mu.HPDO <- apply(mu, 2, HDPI, prob=0.89)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)
mu,mea
mu.mean
mu.HPDI
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))
# plot MAP line
lines(weight.seq, mu.mean)
# add in shaded region
shade(mu.HPDI, weight.seq)
sim.height <- sim(m4.3, data = list(weight = weight.seq))
str(sim.height)
height.PI <- apply(sim.heigh, 2, PI, prob = 0.89)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
plot(height~weight, d2, col = col.alpha(rangi2, 0.5))
# draw MAP
lines(weight.seq, mu.mean)
# drar HPDI region
shade(mu.HPDI, weight.seq)
# drar PI region for simulated heights
shade(height.PI, weight.seq)
rm(list = ls())
data(Howell1)
d <- Howell1
str(d)
plot(height~weight)
plot(height~weight, d)
d$weight.s <- (d$weight - mean(d$weight))/sd(d$weight)
mean(d$weight.s)
d$weight.s2 <- d$weight.s^2
m4.5 <- map(
alist(
height ~ dnorm(mu, sigma),
mu <- a + b1*weight.s + b2*weight.s2 ,
a ~ dnorm(178, 100),
b1 ~ dnorm(0, 10),
b2 ~ dnorm(0, 10),
sigma ~ dunif(0, 50)
),
data = d)
precis(m4.5)
weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight.s=weight.seq, weight.s2=weight.seq^2)
mu <- link(m4.5, data = pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
sim.height <- sim(m4.5, data = pred_dat)
height.PI <- apply(sim.height, 2, PI, prob = 0.89)
plot(height ~ weight.s, d, col = col.alpha(rani2,0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
plot(height ~ weight.s, d, col = col.alpha(rangi2,0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
d$weight.s3 <- d$weight.s^3
# add in the extra term again
m4.6 <- mapply(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b1*weight.s + b2*weight.s2 + b3*weight.s3,
a ~ dnorm(178,100),
b1 ~ dnorm(0,10),
b2 ~ dnorm(0,10),
b3 ~ dnorm(0,10),
sigma ~ dunif(0,50)
),
data = d)
m4.6 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b1*weight.s + b2*weight.s2 + b3*weight.s3,
a ~ dnorm(178,100),
b1 ~ dnorm(0,10),
b2 ~ dnorm(0,10),
b3 ~ dnorm(0,10),
sigma ~ dunif(0,50)
),
data = d)
test <- replicate(1e4, prod(0, 0, 10))
test
test <- replicate(1e4, prod(12, 0, 10))
head(test)
sim_mu <- rnorm(1e4, 0, 10)
sim_sig <- runif(1e4, 0, 10)
sim_prior <- rnorm(1e4, sim_mu, sim_sig)
sim_mu <- rnorm(1e4, 0, 10)
sim_sig <- runif(1e4, 0, 10)
sim_prior <- rnorm(1e4, sim_mu, sim_sig)
dens(sim_prior)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18,]
rm(d)
d2$weight.s <- (d2$weight - mean(d2$weight)) / sd(d2$weight)
m4.4 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.c,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
m4.4 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.s,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.s,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
post <- extract.samples(m4.H1)
mu_at_50 <- post$a + post$b * 50
mu_at_50
d2$weight.c <- d2$weight - mean(d2$weight)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.c,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
# now let's extract some stuff
post <- extract.samples(m4.H1)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight.c,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
post <- extract.samples(m4.H1)
mu_at_50 <- post$a + post$b * 50
mean(mu_at_50)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight,
a ~ dnorm(156,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
# now let's extract some stuff
post <- extract.samples(m4.H1)
mu_at_50 <- post$a + post$b * 50
mean(mu_at_50)
m4.H1 <- map(
alist(
height ~ dnorm(mu,sigma),
mu <- a + b*weight,
a ~ dnorm(178,100),
b ~ dnorm(0,10),
sigma ~ dunif(0,50)
) ,
data = d2)
# now let's extract some stuff
post <- extract.samples(m4.H1)
mu_at_50 <- post$a + post$b * 50
mean(mu_at_50)
weights_to_check <- c(46.95, 43.72, 64.78, 32.59, 54.63)
d = NULL
d = rbind(d, data.frame(weights_to_check))
d
for(i in weights_to_check)
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
d = rbind(d, data.frame(weights_to_check, Expected_height, Int89))
rm(d)
d = NULL
for(i in weights_to_check)
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
d = rbind(d, data.frame(weights_to_check, Expected_height, Int89))
temp <- post$a + post$b * 50
Int89 <- HPDI(temp, prob = 0.89)
Int89
Int89[1]
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
d = rbind(d, data.frame(weights_to_check, Expected_height, Int89[1], Int89[2]))
}
rm(d)
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
d = rbind(d, data.frame(weights_to_check, Expected_height, lower, upper))
}
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
d = rbind(d, data.frame(weights_to_check[i], Expected_height, lower, upper))
}
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
d = rbind(d, data.frame(i, Expected_height, lower, upper))
}
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
d = rbind(d, data.frame(i, Expected_height, lower, upper))
}
head(d)
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
head(d)
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1,]
upper <- Int89[2,]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[,1]
upper <- Int89[,2]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
d = NULL
for(i in weights_to_check) {
temp <- post$a + post$b * i
Expected_height <- mean(temp)
Int89 <- HPDI(temp, prob = 0.89)
lower <- Int89[1]
upper <- Int89[2]
Weights <- i
d = rbind(d, data.frame(Weights, Expected_height, lower, upper))
}
temp <- post$a + post$b * 50
HDPI(temp)
HPDI(temp)
HPDI(temp)[1]
HPDI(temp)[2]
head(d)
d
data(islandsDistMatrix)
Dmat <- islandsDistMatrix
library(rethinking)
data(islandsDistMatrix)
Dmat <- islandsDistMatrix
data("Kline2")
d <- Kline2
# tidy
rm(Kline2)
# prep indexes
d$society <- 1:10
m13.7 <- map2stan(
alist(
total_tools ~ dpois(lambda),
log(lambda) <- a + g[society] + bp*logpop,
g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ),
a ~ dnorm(0,10),
bp ~ dnorm(0,1),
etasq ~ dcauchy(0,1),
rhosq ~ dcauchy(0,1)
),
data=list(
total_tools=d$total_tools,
logpop=d$logpop,
society=d$society,
Dmat=islandsDistMatrix),
warmup=2000 , iter=1e4 , chains=4)
precis(m13.7,depth=2)
post <- extract.samples(m13.7)
curve(median(post$etasq)*exp(-median(post$rhosq)*x^2), from = 0, to = 10,
xlab = "dist (thousand km", ylab = "covariance", ylim = c(0,1),
yaxp = c(0,1,4), lwd=2)
curve(median(post$etasq)*exp(-median(post$rhosq)*x^2), from = 0, to = 10,
xlab = "dist (thousand km", ylab = "covariance", ylim = c(0,1),
yaxp = c(0,1,4), lwd=2)
# plot 100 function samples
for( i in 1:100)
curve(post$etasq[i]*exp(-post$rhosq[i]*x^2), add = T,
col = col.alpha("black", 0.2))
dev.off()
K <- matrix(0, nrow = 10, ncol = 10)
for(i in 1:10)
for(j in 1:10)
K[i,j] <- median(post$etasq) *
exp(-median(post$rhosq) *
islandsDistMatrix[i,j]^2)
diag(K) <- median(post$etasq) + 0.01
Rho <- round(cov2cor(K), 2)
# add names for convenience
colnames(Rho) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
rownames(Rho) <- colnames(Rho)
Rhow
Rho
psize <- d$logpop/max(d$logpop)
psize <- exp(psize*1.5)-2
# plot raw data and labels
plot(d$lon2, d$lat, xlab = "longitude", ylab = "latitude",
col = randi2, cex = psize, pch = 16, xlim = c(-50,30))
labels <- as.character(d$culture)
text(d$lon2, d$lat, labels = labels, cex = 0.7, pos = c(2,4,3,3,4,1,3,2,4,2))
# overlay lines
for(i in 1:10)
for(j in 1:10)
if(i < j)
lines(c(d$lon2[i],d$lon2[j]), c(d$lat[i], d$lat[j]),
lwd=2, col = col.alpha("black", Rho[i,j]^2))
psize <- d$logpop/max(d$logpop)
psize <- exp(psize*1.5)-2
# plot raw data and labels
plot(d$lon2, d$lat, xlab = "longitude", ylab = "latitude",
col = randi2, cex = psize, pch = 16, xlim = c(-50,30))
labels <- as.character(d$culture)
text(d$lon2, d$lat, labels = labels, cex = 0.7, pos = c(2,4,3,3,4,1,3,2,4,2))
# overlay lines
for(i in 1:10)
for(j in 1:10)
if(i < j)
lines(c(d$lon2[i],d$lon2[j]), c(d$lat[i], d$lat[j]),
lwd=2, col = col.alpha("black", Rho[i,j]^2))
plot(d$lon2, d$lat, xlab = "longitude", ylab = "latitude",
col = rangi2, cex = psize, pch = 16, xlim = c(-50,30))
labels <- as.character(d$culture)
text(d$lon2, d$lat, labels = labels, cex = 0.7, pos = c(2,4,3,3,4,1,3,2,4,2))
# overlay lines
for(i in 1:10)
for(j in 1:10)
if(i < j)
lines(c(d$lon2[i],d$lon2[j]), c(d$lat[i], d$lat[j]),
lwd=2, col = col.alpha("black", Rho[i,j]^2))
logpop.seq <- seq( from=6 , to=14 , length.out=30 )
lambda <- sapply( logpop.seq ,
function(lp) exp( post$a + post$bp*lp ) )
lambda.median <- apply( lambda , 2 , median )
lambda.PI80 <- apply( lambda , 2 , PI , prob=0.8 )
# plot raw data and labels
plot( d$logpop ,
d$total_tools ,
col=rangi2 ,
cex=psize ,
pch=16 ,
xlab="log population" ,
ylab="total tools" )
text( d$logpop ,
d$total_tools ,
labels=labels ,
cex=0.7 ,
pos=c(4,3,4,2,2,1,4,4,4,2) )
# display posterior predictions
lines( logpop.seq ,
lambda.median ,
lty=2 )
lines( logpop.seq ,
lambda.PI80[1,] ,
lty=2 )
lines( logpop.seq ,
lambda.PI80[2,] ,
lty=2 )
# overlay correlations
for( i in 1:10 )
for ( j in 1:10 )
if ( i < j )
lines( c( d$logpop[i],d$logpop[j] ) ,
c( d$total_tools[i],d$total_tools[j] ) ,
lwd=2 , col=col.alpha("black",Rho[i,j]^2) )
setwd("F:/Uni/Github/Instructed_Eye_movements")
#### Plotting Script ####
# Level 4 Thesis by Elle
# 2017/18
# Written by Warren James
# Script used to make plots of proportion of fixations
# made to the centre or side box(es)
#### libraries needed ####
library(tidyverse)
#### load in data ####
load("scratch/Elle_switch_nar_data")
#### separate datasets ####
# this part is so we have a separate dataset for:
# no instructions/first half
no_inst_1 <- switch_df[switch_df$condition == "No_instructions" &
switch_df$block < 5,]
# no instructions/second half
no_inst_2 <- switch_df[switch_df$condition == "No_instructions" &
switch_df$block > 4,]
# instructions/tutorial
inst_tut <- switch_df[switch_df$condition == "Instructions" &
switch_df$part == 3,]
# instructions/task
inst_tas <- switch_df[switch_df$condition == "Instructions" &
switch_df$part == 2,]
#### Assuming we only want to look at part 2 for each participant ####
df_part2 <- switch_df[switch_df$part == 2,]
#### make plots ####
# setup data.frame/tibble for plots
# centre proportions
temp <- group_by(df_part2, participant,separation,condition)
centre_prop <- summarise(temp, prop_fixated = mean(centre))
centre_prop$box <- "centre"
# side proportions
side_prop <- summarise(temp, prop_fixated = 1 - mean(centre))
side_prop$box <- "side"
# tidy
rm(temp)
# merge data
plt_dat <- rbind(centre_prop, side_prop)
# tidy
rm(centre_prop, side_prop)
# need to add switch point data back in
temp <- group_by(df_part2, participant, condition)
switch_points <- summarise(temp, switch_point = unique(switch_point))
# tidy
rm(temp)
# now to make the plots
prop_plt <- ggplot(data = plt_dat,
aes(x = separation,
y = prop_fixated))
prop_plt <- prop_plt + geom_area(aes(colour = box,
fill = box),
position = "stack")
prop_plt <- prop_plt + geom_vline(data = switch_points,
aes(xintercept = as.numeric(switch_point)),
linetype = "dashed")
prop_plt <- prop_plt + facet_wrap(~condition + participant)
prop_plt
setwd("F:/Uni/Github/TargetAbsentStoppingRules/analysis")
library(brms)
library(rstan)
#### set options ####
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
#### load in data ####
load("scratch/processed_data_nar_TA.rda")
# turn tibble into data.frame for map
df <- as.data.frame(df_TA)
# tidy
rm(df_TA)
get_prior(rt ~ theta + (1 + theta|participant),
data = df)
setwd("F:/Uni/Github/TargetAbsentStoppingRules/analysis")
load("scratch/models/brm_m3")
m3_effects <- marginal_effects(m3_rt_theta_bt)
plt(m3_effects, plot = TRUE)
plot(m3_effects, plot = TRUE)
library(shinystan)
launch_shinystan(m3_rt_theta_bt)
exp(0.55)
